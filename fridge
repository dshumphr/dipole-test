
######################################################################

2024 Jan 07 21:37:48 (from mygpt.py)


# This is one order of magnitude more complicated than I expected, not
# elegant, slow, hopefully not buggy


def flash_back_time_src(N, H, t0, t1, CL, CH, proba, device):
    # starting flash backs
    fb_start = (torch.rand(N, CH, t1 - t0, device=device) <= proba).long()
    fb_start[:, :, -CL:] = 0
    fb_start[:, :, :CL] = 0

    # Remove series longer than CL
    fb_body = fb_start.clone()
    fb_body[:, :, CL + 1 :] -= fb_start[:, :, : -(CL + 1)]
    fb_body = fb_body.cumsum(dim=2)
    fb_start = fb_start * (fb_body == 1)

    # Set a origin source time (starting time of the chunck to copy
    # here) We set it as the current time minus a multiple of CL to be
    # consistent with the "rolling" caterpillar
    t = torch.arange(fb_start.size(2), device=fb_start.device)[None, None, :]
    src_time = fb_start * (
        t
        - CL
        * (
            1
            + (
                torch.rand(fb_start.size(), device=fb_start.device) * (t // CL - 1)
            ).long()
        )
    )
    src_time[:, :, CL:] -= src_time.clone()[:, :, :-CL]
    src_time = src_time.cumsum(dim=2)

    src_head = fb_start * torch.randint(H, fb_start.size(), device=fb_start.device)
    src_head[:, :, CL:] -= src_head.clone()[:, :, :-CL]
    src_head = src_head.cumsum(dim=2)

    # combine
    src_delta = fb_start.clone()
    src_delta[:, :, CL:] -= fb_start[:, :, :-CL]
    src_delta = src_delta.cumsum(dim=2)
    src_delta[:, :, CL:] -= CL * fb_start[:, :, :-CL]
    src_time += src_delta.cumsum(dim=2) - 1

    return src_time, src_head


def insert_flash_back(rec_V, V, rec_K, K, t0, t1, CL, proba):
    N, H, CH = V.size(0), V.size(1), rec_V.size(1)

    fbt, fbh = flash_back_time_src(N, H, t0, t1, CL, CH, proba, rec_V.device)

    fbt_V = fbt[:, :, :, None]
    fbh_V = fbh[:, :, :, None]
    t = fbt_V.clamp(min=0)
    n = torch.arange(V.size(0), device=V.device)[:, None, None, None]
    d = torch.arange(V.size(3), device=V.device)[None, None, None, :]
    q = V[:, :, t0:t1][n, fbh_V, t, d]
    rec_V[:, :, t0:t1] = q * (fbt_V >= 0) + rec_V[:, :, t0:t1] * (fbt_V < 0)

    fbt_K = fbt[:, :, :, None]
    fbh_K = fbh[:, :, :, None]
    t = fbt_K.clamp(min=0)
    n = torch.arange(K.size(0), device=K.device)[:, None, None, None]
    d = torch.arange(K.size(3), device=K.device)[None, None, None, :]
    q = K[:, :, t0:t1][n, fbh_K, t, d]
    rec_K[:, :, t0:t1] = q * (fbt_K >= 0) + rec_K[:, :, t0:t1] * (fbt_K < 0)


######################################################################

######################################################################

2024 Jan 07 21:38:11 (from mygpt.py)

            # insert_flash_back(self.rec_V,V,self.rec_K,K,t0,t1,CL,proba=self.proba_flashback / CL,)


######################################################################

2024 Jan 09 14:24:42 (from mygpt.py)

            # This piece of code makes the assumption that there is
            # nothing informative before t0, otherwise we'd have to
            # implement a cache for V and K too. This should not be
            # too much of a problem since this is used only during
            # train, where full sequence are available

            # n = torch.arange(N, device=X.device)[:, None, None, None]
            # t = torch.arange(t0, t1, device=X.device)[None, None, :, None]
            # dv = torch.arange(DV, device=X.device)[None, None, None, :]
            # dk = torch.arange(DK, device=X.device)[None, None, None, :]

            # u = (
                # torch.rand(N, CH, t1 - t0, 1, device=X.device).mul(t).long() // CL
            # ) * CL

            # src_time = t - u - t0
            # src_head = torch.randint(H, (N, CH, t1 - t0, 1), device=X.device)

            # mask = (
                # torch.rand(N, CH, t1 - t0, DV, device=X.device) <= self.proba_flashback
            # ).long()

            # self.rec_V[:, :, t0:t1] = (
                # mask * V[n, src_head, src_time, dv]
                # + (1 - mask) * self.rec_V[:, :, t0:t1]
            # )

            # self.rec_K[:, :, t0:t1] = (
                # mask * K[n, src_head, src_time, dk]
                # + (1 - mask) * self.rec_K[:, :, t0:t1]
            # )

######################################################################

2024 Jan 10 08:10:39 (from mygpt.py)

        # That was a bad idea
        # G = F.dropout(G, self.attention_dropout, self.training)

